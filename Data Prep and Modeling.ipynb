{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sammlung aller importe\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from transformers import TFMT5ForConditionalGeneration, MT5Tokenizer, DataCollatorForSeq2Seq\n",
    "from keras.optimizers.legacy import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition der Funktion zum Einlesen aller TXT Dateien aus den Daten\n",
    "def find_and_read_txt_files(directory,string):\n",
    "    # Liste zum Speichern des Inhalts der gefundenen .txt-Dateien und der zugehörigen Dateinamen-Nummer\n",
    "    data = []\n",
    "\n",
    "    # Durchsucht die Ordnerstruktur rekursiv nach .txt-Dateien\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            # Überprüft, ob es eine .txt-Datei ist und das Wort \"source\" im Dateinamen steht\n",
    "            if file.endswith('.txt') and string in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                # Liest den Inhalt der .txt-Datei\n",
    "                with open(file_path, 'r', encoding=\"latin1\") as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                    # Findet die Zahl im Dateinamen mithilfe eines regulären Ausdrucks\n",
    "                    number = re.search(r'[^-]*', file).group()\n",
    "\n",
    "                    # Fügt den Inhalt und die Zahl dem Datensatz hinzu\n",
    "                    data.append([content, number])\n",
    "\n",
    "    # Erstellt einen DataFrame aus den gesammelten Daten\n",
    "    df = pd.DataFrame(data, columns=[string, 'number'])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_and_read_txt_files(directory,string):\n",
    "    # Liste zum Speichern des Inhalts der gefundenen .txt-Dateien und der zugehörigen Dateinamen-Nummer\n",
    "    data = []\n",
    "\n",
    "    # Durchsucht die Ordnerstruktur rekursiv nach .txt-Dateien\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            # Überprüft, ob es eine .txt-Datei ist und das Wort \"source\" im Dateinamen steht\n",
    "            if file.endswith('.txt') and string in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                # Liest den Inhalt der .txt-Datei\n",
    "                with open(file_path, 'r', encoding=\"latin1\") as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                    # Findet die Zahl im Dateinamen mithilfe eines regulären Ausdrucks\n",
    "                    number = re.search(r'?P<id>\\d+', file).group()\n",
    "\n",
    "                    # Fügt den Inhalt und die Zahl dem Datensatz hinzu\n",
    "                    data.append([content, number])\n",
    "\n",
    "    # Erstellt einen DataFrame aus den gesammelten Daten\n",
    "    df = pd.DataFrame(data, columns=[string, 'number'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/Users/huyduc/Documents/GitHub/Hettich/FileArchive_EDIFACT 2\"  # Pfad zum Übergeordneten Ordner\n",
    "df_target = find_and_read_txt_files(directory_path,'target')\n",
    "df_source = find_and_read_txt_files(directory_path,'source')\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16551"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.number.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source    0\n",
       "number    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_source.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16551"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_target.number.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_source, df_target, on='number', how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtern der Daten nach Länge"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie Frederik bereits in seiner Mail erwähnt hat, hat OpenAI die Limitierung Strings mit einer Länge von insgesamt 25000 Zeichen pro Zeile nicht zu tokenisieren. Das heißt wir filtern alle Bestellungen raus, wo die Zeichenkette aus source und target zusammen größer gleich 25000 sind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ermitteln der Zeichenlänge aus source und target pro Zeile\n",
    "df_merged[\"len_source\"]=df_merged[\"source\"].str.len() \n",
    "df_merged[\"len_target\"]=df_merged[\"target\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>number</th>\n",
       "      <th>target</th>\n",
       "      <th>len_source</th>\n",
       "      <th>len_target</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNB+UNOA:3+4025249000006:14+4008057000000:14+2...</td>\n",
       "      <td>EXT_96861056</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>3144</td>\n",
       "      <td>7655</td>\n",
       "      <td>10799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNB+UNOA:3+4025249000006:14+4008057000000:14+2...</td>\n",
       "      <td>EXT_96863929</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;MULTIPL...</td>\n",
       "      <td>10795</td>\n",
       "      <td>25066</td>\n",
       "      <td>35861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNA:+.? 'UNB+UNOD:3+4025248000144:14+400805700...</td>\n",
       "      <td>EXT_96852746</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>2994</td>\n",
       "      <td>5748</td>\n",
       "      <td>8742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNB+UNOA:3+4025249000006:14+4008057000000:14+2...</td>\n",
       "      <td>EXT_96849879</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>6005</td>\n",
       "      <td>13601</td>\n",
       "      <td>19606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNB+UNOA:3+4025249000006:14+4008057000000:14+2...</td>\n",
       "      <td>EXT_96854582</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>2152</td>\n",
       "      <td>5324</td>\n",
       "      <td>7476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16266</th>\n",
       "      <td>UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...</td>\n",
       "      <td>EXT_100534771</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>1411</td>\n",
       "      <td>3175</td>\n",
       "      <td>4586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16267</th>\n",
       "      <td>UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...</td>\n",
       "      <td>EXT_100533946</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>1299</td>\n",
       "      <td>3035</td>\n",
       "      <td>4334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16268</th>\n",
       "      <td>UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...</td>\n",
       "      <td>EXT_100533539</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>1476</td>\n",
       "      <td>3378</td>\n",
       "      <td>4854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16269</th>\n",
       "      <td>UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...</td>\n",
       "      <td>EXT_100535514</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>1298</td>\n",
       "      <td>3102</td>\n",
       "      <td>4400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16270</th>\n",
       "      <td>UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...</td>\n",
       "      <td>EXT_100531164</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>1281</td>\n",
       "      <td>3031</td>\n",
       "      <td>4312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16271 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source         number  \\\n",
       "0      UNB+UNOA:3+4025249000006:14+4008057000000:14+2...   EXT_96861056   \n",
       "1      UNB+UNOA:3+4025249000006:14+4008057000000:14+2...   EXT_96863929   \n",
       "2      UNA:+.? 'UNB+UNOD:3+4025248000144:14+400805700...   EXT_96852746   \n",
       "3      UNB+UNOA:3+4025249000006:14+4008057000000:14+2...   EXT_96849879   \n",
       "4      UNB+UNOA:3+4025249000006:14+4008057000000:14+2...   EXT_96854582   \n",
       "...                                                  ...            ...   \n",
       "16266  UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...  EXT_100534771   \n",
       "16267  UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...  EXT_100533946   \n",
       "16268  UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...  EXT_100533539   \n",
       "16269  UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...  EXT_100535514   \n",
       "16270  UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...  EXT_100531164   \n",
       "\n",
       "                                                  target  len_source  \\\n",
       "0      <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        3144   \n",
       "1      <?xml version=\"1.0\" encoding=\"UTF-8\"?><MULTIPL...       10795   \n",
       "2      <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        2994   \n",
       "3      <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        6005   \n",
       "4      <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        2152   \n",
       "...                                                  ...         ...   \n",
       "16266  <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        1411   \n",
       "16267  <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        1299   \n",
       "16268  <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        1476   \n",
       "16269  <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        1298   \n",
       "16270  <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        1281   \n",
       "\n",
       "       len_target    len  \n",
       "0            7655  10799  \n",
       "1           25066  35861  \n",
       "2            5748   8742  \n",
       "3           13601  19606  \n",
       "4            5324   7476  \n",
       "...           ...    ...  \n",
       "16266        3175   4586  \n",
       "16267        3035   4334  \n",
       "16268        3378   4854  \n",
       "16269        3102   4400  \n",
       "16270        3031   4312  \n",
       "\n",
       "[16271 rows x 6 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Aufaddieren dieser Zahlen\n",
    "df_merged[\"len\"]=df_merged[\"len_source\"]+df_merged[\"len_target\"]\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>number</th>\n",
       "      <th>target</th>\n",
       "      <th>len_source</th>\n",
       "      <th>len_target</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNB+UNOA:3+4025249000006:14+4008057000000:14+2...</td>\n",
       "      <td>EXT_96861056</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>3144</td>\n",
       "      <td>7655</td>\n",
       "      <td>10799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNA:+.? 'UNB+UNOD:3+4025248000144:14+400805700...</td>\n",
       "      <td>EXT_96852746</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>2994</td>\n",
       "      <td>5748</td>\n",
       "      <td>8742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNB+UNOA:3+4025249000006:14+4008057000000:14+2...</td>\n",
       "      <td>EXT_96849879</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>6005</td>\n",
       "      <td>13601</td>\n",
       "      <td>19606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNB+UNOA:3+4025249000006:14+4008057000000:14+2...</td>\n",
       "      <td>EXT_96854582</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>2152</td>\n",
       "      <td>5324</td>\n",
       "      <td>7476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNB+UNOA:3+4025249000006:14+4008057000000:14+2...</td>\n",
       "      <td>EXT_96870001</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;MULTIPL...</td>\n",
       "      <td>6571</td>\n",
       "      <td>16432</td>\n",
       "      <td>23003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14069</th>\n",
       "      <td>UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...</td>\n",
       "      <td>EXT_100534771</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>1411</td>\n",
       "      <td>3175</td>\n",
       "      <td>4586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14070</th>\n",
       "      <td>UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...</td>\n",
       "      <td>EXT_100533946</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>1299</td>\n",
       "      <td>3035</td>\n",
       "      <td>4334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14071</th>\n",
       "      <td>UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...</td>\n",
       "      <td>EXT_100533539</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>1476</td>\n",
       "      <td>3378</td>\n",
       "      <td>4854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14072</th>\n",
       "      <td>UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...</td>\n",
       "      <td>EXT_100535514</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>1298</td>\n",
       "      <td>3102</td>\n",
       "      <td>4400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14073</th>\n",
       "      <td>UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...</td>\n",
       "      <td>EXT_100531164</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "      <td>1281</td>\n",
       "      <td>3031</td>\n",
       "      <td>4312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14074 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  source         number  \\\n",
       "0      UNB+UNOA:3+4025249000006:14+4008057000000:14+2...   EXT_96861056   \n",
       "1      UNA:+.? 'UNB+UNOD:3+4025248000144:14+400805700...   EXT_96852746   \n",
       "2      UNB+UNOA:3+4025249000006:14+4008057000000:14+2...   EXT_96849879   \n",
       "3      UNB+UNOA:3+4025249000006:14+4008057000000:14+2...   EXT_96854582   \n",
       "4      UNB+UNOA:3+4025249000006:14+4008057000000:14+2...   EXT_96870001   \n",
       "...                                                  ...            ...   \n",
       "14069  UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...  EXT_100534771   \n",
       "14070  UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...  EXT_100533946   \n",
       "14071  UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...  EXT_100533539   \n",
       "14072  UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...  EXT_100535514   \n",
       "14073  UNA:+.? 'UNB+UNOD:2+4306517008994:14+400805700...  EXT_100531164   \n",
       "\n",
       "                                                  target  len_source  \\\n",
       "0      <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        3144   \n",
       "1      <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        2994   \n",
       "2      <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        6005   \n",
       "3      <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        2152   \n",
       "4      <?xml version=\"1.0\" encoding=\"UTF-8\"?><MULTIPL...        6571   \n",
       "...                                                  ...         ...   \n",
       "14069  <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        1411   \n",
       "14070  <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        1299   \n",
       "14071  <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        1476   \n",
       "14072  <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        1298   \n",
       "14073  <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...        1281   \n",
       "\n",
       "       len_target    len  \n",
       "0            7655  10799  \n",
       "1            5748   8742  \n",
       "2           13601  19606  \n",
       "3            5324   7476  \n",
       "4           16432  23003  \n",
       "...           ...    ...  \n",
       "14069        3175   4586  \n",
       "14070        3035   4334  \n",
       "14071        3378   4854  \n",
       "14072        3102   4400  \n",
       "14073        3031   4312  \n",
       "\n",
       "[14074 rows x 6 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rausfiltern der Dateneinträge, wo len>=25000 ist\n",
    "df_stripped=df_merged[df_merged[\"len\"]<25000].reset_index(drop=True)\n",
    "df_stripped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speichern des Dateneintrages\n",
    "df_stripped.to_csv(\"stripped_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source        0\n",
       "number        0\n",
       "target        0\n",
       "len_source    0\n",
       "len_target    0\n",
       "len           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged2=df_merged.sample(1250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged2=df_merged.sample(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged2=df_merged2.sample(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufteilen des Datensatzes in Trainings und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(df_merged2[\"source\"], df_merged2[\"target\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_stripped[\"source\"], df_stripped[\"target\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.DataFrame(columns=[\"source\",\"target\"])#y_train.to_csv(\"train.csv\")\n",
    "test=pd.DataFrame(columns=[\"source\",\"target\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speicherung der Daten separat als Trainings und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train[\"source\"]=X_train\n",
    "train[\"target\"]=y_train\n",
    "train=train.reset_index(drop=True)\n",
    "train.to_csv(\"stripped_train.csv\")\n",
    "#train.to_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"source\"]=X_test\n",
    "test[\"target\"]=y_test\n",
    "test.to_csv(\"stripped_test.csv\")\n",
    "#test.to_csv(\"test.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ab dieser Überschrift alles ignorieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "model = TFMT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://drive.google.com/file/d/1wLHcRo5rwpsen4pFLSvCX3WUGBKHFqgx/view?usp=sharing' \n",
    "path = 'https://drive.google.com/uc?export=download&id='+url.split('/')[-2]\n",
    "orderlines = pd.read_csv(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"csv\", data_files=path)\n",
    "dataset = dataset[\"train\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    padding = \"max_length\"\n",
    "    max_length = 200\n",
    "\n",
    "    inputs = [ex for ex in examples[\"source\"]]\n",
    "    targets = [ex for ex in examples[\"target\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=padding, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=max_length, padding=padding, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.map(preprocess_function, batched=True, desc=\"Running tokenizer\")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=tokenizer.pad_token_id,\n",
    "    pad_to_multiple_of=64,\n",
    "    return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    train_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(3e-5))\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "model.fit(tf_train_dataset, epochs=10, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(train[\"source\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read\n",
    "#df = pd.read_parquet('/Users/huyduc/Downloads/train.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#```\n",
    "\n",
    "#3. **Tokenisierung**:\n",
    "#Da es sich nicht um eine Standardübersetzung handelt, werden wir den Präfix für die Eingabe weglassen und einfach die Eingabe- und Ausgabetexte tokenisieren.\n",
    "\n",
    "#```python\n",
    "#from transformers import MT5Tokenizer\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "\n",
    "def tokenize_data(edifact_texts, idoc_texts):\n",
    "    src_tokenized = tokenizer(edifact_texts, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    tgt_tokenized = tokenizer(idoc_texts, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    return {\"input_ids\": src_tokenized['input_ids'], \"attention_mask\": src_tokenized['attention_mask'], \"labels\": tgt_tokenized['input_ids']}\n",
    "\n",
    "tokenized_data = tokenize_data(edifact_texts, idoc_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MT5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    output_dir='./results',\n",
    "    save_total_limit=3,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Angenommen, du teilst deinen tokenisierten Daten in Trainings- und Validierungssets auf\n",
    "train_data = tokenized_data[:int(0.8*len(tokenized_data))]\n",
    "val_data = tokenized_data[int(0.8*len(tokenized_data)):]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "#```\n",
    "\n",
    "#Stellen Sie sicher, dass Sie genügend Datenpunkte für das Training und die Validierung haben und dass Ihre Daten sauber und korrekt formatiert sind. Transformer-basierte Modelle können mit unstrukturierten Daten wie EDIFACT und IDOC umgehen, benötigen aber eine ausreichende Datenmenge und Ressourcen, um gut zu funktionieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anacondaforarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
