{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sammlung aller importe\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset\n",
    "from transformers import TFMT5ForConditionalGeneration, MT5Tokenizer, DataCollatorForSeq2Seq\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition der Funktion zum Einlesen aller TXT Dateien aus den Daten\n",
    "def find_and_read_txt_files(directory,string):\n",
    "    # Liste zum Speichern des Inhalts der gefundenen .txt-Dateien und der zugehörigen Dateinamen-Nummer\n",
    "    data = []\n",
    "\n",
    "    # Durchsucht die Ordnerstruktur rekursiv nach .txt-Dateien\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            # Überprüft, ob es eine .txt-Datei ist und das Wort \"source\" im Dateinamen steht\n",
    "            if file.endswith('.txt') and string in file:\n",
    "                file_path = os.path.join(root, file)\n",
    "\n",
    "                # Liest den Inhalt der .txt-Datei\n",
    "                with open(file_path, 'r', encoding=\"latin1\") as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                    # Findet die Zahl im Dateinamen mithilfe eines regulären Ausdrucks\n",
    "                    number = re.search(r'\\d+', file).group()\n",
    "\n",
    "                    # Fügt den Inhalt und die Zahl dem Datensatz hinzu\n",
    "                    data.append([content, number])\n",
    "\n",
    "    # Erstellt einen DataFrame aus den gesammelten Daten\n",
    "    df = pd.DataFrame(data, columns=[string, 'number'])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"/Users/huyduc/Documents/GitHub/Hettich/FileArchive_EDIFACT 2\"  # Pfad zum Übergeordneten Ordner\n",
    "df_target = find_and_read_txt_files(directory_path,'target')\n",
    "df_source = find_and_read_txt_files(directory_path,'source')\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df_target, df_source, on='number', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target      0\n",
       "number      0\n",
       "source    280\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged2=df_merged.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged2=df_merged2.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_merged2[\"source\"], df_merged2[\"target\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.DataFrame(columns=[\"source\",\"target\"])#y_train.to_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"source\"]=X_train\n",
    "train[\"target\"]=y_train\n",
    "train=train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNA:+.? 'UNB+UNOC:3+3025940000000:14+400805700...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;ORDER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UNA:+.? 'UNB+UNOC:3+4333990000009:14+400805700...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNA:+.? 'UNB+UNOC:3+4399902231817:14+402300900...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;MULTI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UNA:+.? 'UNB+UNOC:3+4304449000000:14:431556399...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;ORDER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UNA:+.? 'UNB+UNOC:3+3025940000000:14+400805700...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;ORDER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>UNA:+.? 'UNB+UNOD:3+4399901757592:14+400805700...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;MULTIPL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>UNA:+.? 'UNB+UNOC:3+4333990000009:14+400805700...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>UNA:+.? 'UNB+UNOC:3+4333990000009:14+400805700...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>UNA:+.? 'UNB+UNOC:3+4250517300001:14+9019970:1...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;ORDERS0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>UNA:+.? 'UNB+UNOC:3+3025940000000:14+400805700...</td>\n",
       "      <td>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;ORDER...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                source  \\\n",
       "0    UNA:+.? 'UNB+UNOC:3+3025940000000:14+400805700...   \n",
       "1    UNA:+.? 'UNB+UNOC:3+4333990000009:14+400805700...   \n",
       "2    UNA:+.? 'UNB+UNOC:3+4399902231817:14+402300900...   \n",
       "3    UNA:+.? 'UNB+UNOC:3+4304449000000:14:431556399...   \n",
       "4    UNA:+.? 'UNB+UNOC:3+3025940000000:14+400805700...   \n",
       "..                                                 ...   \n",
       "795  UNA:+.? 'UNB+UNOD:3+4399901757592:14+400805700...   \n",
       "796  UNA:+.? 'UNB+UNOC:3+4333990000009:14+400805700...   \n",
       "797  UNA:+.? 'UNB+UNOC:3+4333990000009:14+400805700...   \n",
       "798  UNA:+.? 'UNB+UNOC:3+4250517300001:14+9019970:1...   \n",
       "799  UNA:+.? 'UNB+UNOC:3+3025940000000:14+400805700...   \n",
       "\n",
       "                                                target  \n",
       "0    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<ORDER...  \n",
       "1    <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...  \n",
       "2    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<MULTI...  \n",
       "3    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<ORDER...  \n",
       "4    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<ORDER...  \n",
       "..                                                 ...  \n",
       "795  <?xml version=\"1.0\" encoding=\"UTF-8\"?><MULTIPL...  \n",
       "796  <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...  \n",
       "797  <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...  \n",
       "798  <?xml version=\"1.0\" encoding=\"UTF-8\"?><ORDERS0...  \n",
       "799  <?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<ORDER...  \n",
       "\n",
       "[800 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huyduc/anaconda3/envs/anacondaforarm/lib/python3.9/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFMT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFMT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-small\")\n",
    "model = TFMT5ForConditionalGeneration.from_pretrained(\"google/mt5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/huyduc/.cache/huggingface/datasets/csv/default-22411ccb5ea29676/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7d170d2d1944e9887305843556fd8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb59b46dcfce415c9fad86fea713cf6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "975637041a0f49268ea3afe381d14414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/huyduc/.cache/huggingface/datasets/csv/default-22411ccb5ea29676/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huyduc/anaconda3/envs/anacondaforarm/lib/python3.9/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a2f3ede8ce4456a8a76e0825b1e533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"train.csv\")\n",
    "dataset = dataset[\"train\"].shuffle(seed=42)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    padding = \"max_length\"\n",
    "    max_length = 200\n",
    "\n",
    "    inputs = [ex for ex in examples[\"source\"]]\n",
    "    targets = [ex for ex in examples[\"target\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_length, padding=padding, truncation=True)\n",
    "    labels = tokenizer(targets, max_length=max_length, padding=padding, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'source', 'target'],\n",
       "    num_rows: 800\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7613ca8ee240799d8cb1a2fb6efbea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = dataset.map(preprocess_function, batched=True, desc=\"Running tokenizer\")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=tokenizer.pad_token_id,\n",
    "    pad_to_multiple_of=64,\n",
    "    return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_dataset = model.prepare_tf_dataset(\n",
    "    train_dataset,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 21:22:59.234715: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-10-25 21:23:00.259324: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node Adam/AssignAddVariableOp_10.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/100 [..............................] - ETA: 3:21:37 - loss: 29.6417"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=Adam(3e-5))\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "model.fit(tf_train_dataset, epochs=10, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anacondaforarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
